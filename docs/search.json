[
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "Project on Image Tagging Bias",
    "section": "",
    "text": "Introduction\nIn the age of AI and machine learning, vision-based cognitive services (CogS) have become integral to a wide array of applications, such as real-time security, social networks, and smartphone apps. As these services heavily rely on analyzing images of people and inferring emotions from facial expressions, it is crucial to examine their potential biases and ensure that they do not inadvertently perpetuate racial stereotypes.\nWe investigated if there is an inherent racial bias in image tagging emotion analyses services in their reading of emotions as compared to a real person reading emotion. We also wanted to know if this corroborates any previous research on the bias towards black individuals erroneously being perceived as angry or hostile through their expressions.\nUpon analysis, we did not find any significant differences. However, we believe that the lack of significant findings may be due to issues with the dataset, warranting further exploration. Our report underscores the importance of rigorously examining potential biases in AI systems and highlights the need for more robust datasets and research methods to better understand and address these biases. While our current findings may not conclusively prove the existence of inherent biases in EAS, the study emphasizes the ongoing need for continuous evaluation and improvement of AI systems to ensure fairness, transparency, and accuracy in AI applications, ultimately contributing to a more ethical and inclusive technological landscape.\n\n\nData description\nIn our study, we analyzed two datasets to investigate the presence of inherent bias in emotion analysis services (EAS) taken from the Harvard Dataverse.\nThe person-data dataset contains human-generated data collected from participants in the US. Each row represents a different individual tasked with describing the facial emotion of a person in a photograph. The columns include information about the evaluator, such as the date and time of evaluation, the participant’s location (city and state), IP address, and demographic information (race and gender). Additionally, the dataset contains columns for the evaluator’s first and second choices of emotion to describe the person in the photo, the emotion they would not use, and the image identifier.\nThe EAS dataset comprises data generated by the emotion analysis services. Each row represents a unique image, and the columns contain the proportion of evaluators who classified the emotion expressed in the picture across seven different emotions. The first column provides the image identifier, while the subsequent columns display the proportions for the emotions Neutral (N), Happiness, Sadness, Surprise, Fear, Disgust, and Anger.\n(Kyriakou, Kyriakos; Kleanthous, Styliani; Otterbarcher, Jahna; Papadopoulos, George, 2021, “Emotion Bias Dataset (EBD)”, https://doi.org/10.7910/DVN/8MW0RA, Harvard Dataverse, V1, UNF:6:axjl4xEJLBvYutu0VQ0pMQ== [fileUNF] )\n\nCleaned dataset contains information about emotions, race, and gender of 1208 individuals.\n\nEmotions of interest include SADNESS, NEUTRAL, ANGER, FEAR, and HAPPINESS.\nEmotions are represented as percentages of the total emotional response, with values ranging from 0 to 1.\nRace is represented as a categorical variable, with the category from White, Black, Latino, Asian.\nGender is also represented as a categorical variable, with the only category being Male and Female.\n\n\n\n\n\n\nData analysis\n\n\n\nBefore continuing with the data analysis, it is important to highlight some potential issues in the data collection which may then lead to issues with the conclusions.\n\n\n    preds\n     ANGER CONTEMPT DISGUST FEAR HAPPINESS NEUTRAL SADNESS SURPRISE\n  A     51        5       4    0         2      91       1        0\n  F      1        1       0   20        25       6      12       84\n  HC     0        0       0    0       146       7       0        0\n  HO     0        0       0    0       154       0       0        0\n  N      0        0       0    0         1     591       4        1\n\n\nAs you can see here, the emotion categories are slightly inconsistent in the dataset. The actual emotions in the row labels don’t exactly match up with the predicted emotions in the column labels. The options for actual are anger, fear, happy closed, happy open, and neutral. On the other hand, the option for the predicted are anger, contempt, disgust, fear, happiness, neutral, sadness, and surprise. This makes analyzing the data and making proper comparisons slightly more difficult and less straight forward.\n\n\n# A tibble: 4 × 2\n  race   `mean(accuracy)`\n  <chr>             <dbl>\n1 Asian             1    \n2 Black             0.749\n3 Latino            0.991\n4 White             0.759\n\n\n     race    actual   n\n1   Asian   NEUTRAL 109\n2   Black   NEUTRAL 197\n3   Black HAPPINESS 164\n4   Black     ANGER  82\n5   Black      FEAR  83\n6  Latino   NEUTRAL 108\n7   White   NEUTRAL 183\n8   White HAPPINESS 143\n9   White     ANGER  72\n10  White      FEAR  66\n\n\nThe tables above show another potential issue in the dataset. In the first table, you can see that the mean accuracy for the Asian and Latino race categories are suspiciously higher which requires further exploration. In the following table, one can observe that the reasoning for this is that for both categories, the only actual emotion option was “neutral”. This may create bias in the analysis, so for this reason we decided to only compare the Black and White race categories.\n\n\n\n\n\nIn the heat maps above, each tile represents a unique combination of “predicted” and “actual” for a specific race (“Black” or “White”), and the color of the tile represents the count of that combination. The plot is divided into separate panels for each race. As we can see, the counts do not really show anything alarming. For Black, neutral is occasionally confused for neutral, and for both races anger is often confused with neutral. Fear is also confused with surprise for both races.\n\n\n\n\n\nThis visualization uses a bar graph to show the distributions for emotion counts from the person data for black compared to white. There seems to fairly similar distributions between the two races, with slightly higher counts for surprise, happiness, and anger for black women compared to white. However, this does not raise much concern since two out of these three emotions are percieved as positive.\n\n\n\n\n\nThis visualization uses a bar graph to show the distributions for emotion counts from the EAS data for black compared to white. Again, there does not seem to be any alarming differences although we need to conduct tests to confirm this. There does not appear to be a significant difference in any of the emotion rating means when comparing black to white.\n\n\n\n\n\nEvaluation of significance\nHypothesis 1\nNull Hypothesis: There is no statistically significant difference between actual and predicted categories for race.\n\\[\nH_0: p_1 - p_2 = 0\n\\]\nAlternative Hypothesis: There is a statistically significant difference between actual and predicted categories for race.\n\\[\nH_A: p_1 - p_2 \\neq 0\n\\]\n\n\n   category statisitcs     pvalue\n1   NEUTRAL  4.7065990 0.09505501\n2 HAPPINESS  0.9025027 0.34211153\n3     ANGER  4.0391307 0.54379596\n4      FEAR  7.9138975 0.24447953\n\n\nThe output shows the results of the Chi-squared tests for the EAS data. Each row corresponds to a unique category in the actual column (‘NEUTRAL’, ‘HAPPINESS’, ‘ANGER’, ‘FEAR’), and the columns display the Chi-squared statistic and the associated p-value. All of the p-values are greater than 0.05, suggesting that there is not enough evidence to reject the null hypothesis of independence for any of the emotion categories. There is not enough evidence to support that there is a statistically significant difference between actual and predicted categories for race for any emotion in the EAS data.\nHypothesis 2\nFor our first hypothesis, we wanted to determine if there was a significantly higher proportion of “false positive” angry decisions made by the Emotion Analysis Service as compared to real people making the same decisions.\nNull Hypothesis: There is not a significantly higher proportion of “false positives” for incorrect angry decisions made by the EAS for black men as compared to real people making the decision\n\\[\nH_0: p_1 - p_2 = 0\n\\]\nAlternative Hypothesis: There is a significantly higher proportion of “false positives” for incorrect angry decisions made by the EAS for black men as compared to real people making the decision.\n\\[\nH_A: p_1 - p_2 \\neq 0\n\\] For this, we used a difference in proportions test in order to find out if there was a significant staitistical difference between the EAS and person type 1 errors. Our results was as shown below:\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(length(type1_eas), length(type1_person)) out of c(length(eas_data_clean), length(person_data_clean))\nX-squared = 0.15476, df = 1, p-value = 0.694\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.6094210  0.3017287\nsample estimates:\n   prop 1    prop 2 \n0.3846154 0.5384615 \n\n\nFor our hypothesis test, we receive a p-value of 0.694, indicating that we fail to reject the null hypothesis. With a threshold of p < 0.05, we can conclude that there is no significant difference in the type 1 errors of the EAS and people’s decisions when it comes to classifiying black men as angry.\n\n\nInterpretation and conclusions\nBased on the results obtained from the two-sample test, we observe a p-value of 0.694, exceeding the conventional significance threshold of 0.05. This suggests insufficient evidence to reject the null hypothesis, which posits that there is no inherent racial bias in Emotion Analysis Services’ (EAS) interpretation of emotions, compared to human evaluation. The 95% confidence interval for the difference in proportions is -0.5678590 to 0.4000268, which encompasses 0. This lends further weight to our conclusion, suggesting with 95% confidence that the true difference in proportions between the two groups falls within this interval, and therefore there is not a significant divergence in Type I error rates between the groups.\nThe Chi-squared tests performed for the second hypothesis further corroborate our conclusions. With p-values exceeding the alpha level of 0.05 for each emotion, we fail to reject the null hypothesis that there is no statistically significant difference between the actual and predicted categories for race across all emotions in the EAS data. This aligns with our earlier findings, suggesting a lack of inherent racial bias in EAS’s emotion interpretation.\nHowever, it is paramount to acknowledge the potential limitations of our dataset, such as the under-representation of the Asian and Latino demographics and inconsistencies in the emotion variables. Such factors could potentially confound our analysis and merit further exploration.\nLooking ahead, future work should aim to address these limitations. In particular, efforts should be directed towards compiling a more diverse and representative dataset. This would involve not only including more data from underrepresented racial groups but also ensuring that the range of emotions captured is comprehensive and consistently categorized. Furthermore, using a variety of statistical tests and machine learning models could provide a more robust analysis of potential biases. We also recommend performing longitudinal studies to track how the performance of these services evolves over time, as the algorithms learn and adapt.\nWhile our findings do not conclusively indicate the presence of inherent racial bias in EAS, they underscore the importance of continuous evaluation and vigilance. This is crucial in ensuring that as these systems evolve, they do so in a manner that is fair, transparent, and accurate, contributing to an ethical and inclusive technological landscape.\n\n\nLimitations\nThere are potential limitations based on the research design:\n\nThere is under-representation of the Asian and Latino demographics and inconsistencies in the emotion variables. This could potentially confound our analysis and merit further exploration.\nInconsistency in the emotion variables; the predicted and accurate options are not the same, potentially confounding our analysis.\nThe sample sizes (1207-1845 observations) used for the analysis may be too small to generalize conclusions about the population. This might lead to the sample that is not representative of the larger population.\nThe study relies heavily on the accuracy of the image tagging and emotion analysis services used. If these services are flawed, the results of the study may be inaccurate or unreliable.\nThe study is limited to the direct emotion variables included in the analysis. There may be other factors that influence emotion recognition that are not accounted for in the study, such as the lighting and picture quality.\nBias could exist, but could be the same for both the EAS and real decisions as the service could be trained on biased data which would also result in no statistical difference.\n\n\n\nAcknowledgments\nThe heat-map was inspired by material from “Intro to ML - INFO 1998” taught by the Cornell Data Science project team."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This project was developed by Team Extraordinary For INFO 2950: Introduction to Data Science at Cornell University. The team is comprised of the following team members.\n\nTeam member 1: Kai Barker, Sophomore Information Science Major. Interested in sports and psychology.\nTeam member 2: Nicole Gerber, Sophomore Information Science Major. Interested in health and ML.\nTeam member 3: Isabella Benenati, Sophomore Information Science Major. Interested in film and design.\nTeam member 4: Baotao Yao, Junior Computer Science and Econ major. Interested in economics.\nTeam member 5: Darrel Dartey, Junior Information Science major. Interested in sports."
  },
  {
    "objectID": "appendicies.html",
    "href": "appendicies.html",
    "title": "Project title",
    "section": "",
    "text": "Data cleaning\n\nlibrary(ggplot2)\nlibrary(readxl)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(skimr)\n\n\n# read in the data\nperson_data <- read_excel(\"data/PERSON_DATASET.xlsx\")\n\n# Drop unnecessary columns\ncolumns_to_drop <- c(\"_trust\", \"_country\", \"_created_at\",\"_started_at\", \"_ip\", \n                     \"1st_choice_gold\", \"2nd_choice_gold\", \"_channel\",\n                     \"i_would_not_use_gold\",\"select_your_gender_gold\",\n                     \"select_your_race_gold\", \"_unit_id\", \"_tainted\", \"_id\")\nperson_data_clean <- person_data |>\n  select(-one_of(columns_to_drop))\n\n# Rename columns for consistency\ncolumn_names <- c(\"id\", \"city\", \"region\", \"first_choice\", \"second_choice\",\n                  \"would_not_use\", \"gender\", \"race\", \"image_url\")\n\ncolnames(person_data_clean) <- column_names\n\n# Step 5: Clean up missing or inconsistent data\nperson_data_clean <- person_data_clean |>\n  mutate(\n    gender = recode(gender, \"NA\" = NA_character_, \"I don't know\" = \"Other\"),\n    race = recode(race, \"NA\" = NA_character_),\n    target = str_sub(image_url, 12, 17),\n    emotion = if_else((str_sub(image_url, 34, 34) == \"H\"), \n              str_sub(image_url, 34, 35), str_sub(image_url, 34, 34))\n    ) \n\nperson_data_clean <- person_data_clean |>\n  select(-image_url) \n\nperson_data_clean <- person_data_clean |>\n  mutate(\n    image_race = case_when(\n      str_sub(target, 1, 1) == \"A\" ~ \"Asian\",\n      str_sub(target, 1, 1) == \"B\" ~ \"Black\",\n      str_sub(target, 1, 1) == \"L\" ~ \"Latino\",\n      str_sub(target, 1, 1) == \"W\" ~ \"White\"\n    ),\n    image_gender = case_when(\n      str_sub(target, 2, 2) == \"M\" ~ \"Male\",\n      str_sub(target, 2, 2) == \"F\" ~ \"Female\"\n    )\n  )\n\n# make sure everything is in english and has same capitalization\nperson_data_clean <- person_data_clean |>\n  mutate(\n    gender = str_to_lower(gender),\n    race = str_to_lower(race)\n  ) |>\n  mutate(\n    gender = case_when(\n      gender %in% c(\"masculino\", \"male\") ~ \"male\",\n      gender %in% c(\"hembra\", \"female\") ~ \"female\",\n      TRUE ~ \"other\"\n    ),\n    race = case_when(\n      race %in% c(\"blanco\", \"white\") ~ \"white\",\n      race %in% c(\"negro\", \"black\") ~ \"black\",\n      race %in% c(\"otro\", \"other\") ~ \"other\",\n      TRUE ~ race\n    )\n  )\n\nperson_data_clean <- person_data_clean |>\n  mutate(emotion = case_when(\n    emotion == \"N\" ~ \"Neutral\",\n    emotion %in% c(\"HO\", \"HC\") ~ \"Happiness\",\n    emotion == \"A\" ~ \"Anger\",\n    emotion == \"F\" ~ \"Fear\",\n    TRUE ~ emotion))\n  \nwrite.csv(person_data_clean, \"data/person_data_clean.csv\")\n\nperson_data_clean\n\n# A tibble: 1,845 × 12\n         id city  region   first_choice second_choice would_not_use gender race \n      <dbl> <chr> <chr>    <chr>        <chr>         <chr>         <chr>  <chr>\n 1  1856261 FL    Palm Ci… Disgust      Surprise      Happiness     female white\n 2 29885393 LA    Lake Ch… Fear         Surprise      Happiness     female white\n 3  6432269 OH    Macedon… Sadness      Anger         Happiness     female white\n 4  4316379 NY    Bronx    Happiness    Neutral       Anger         female white\n 5 45264713 FL    Miami    Happiness    Neutral       Sadness       female lati…\n 6  9559045 OK    Jenks    Happiness    Happiness     Anger         female white\n 7 44044795 AZ    Lake Ha… Fear         Sadness       Happiness     male   white\n 8 27215203 OK    Tulsa    Disgust      Fear          Happiness     female other\n 9 44788207 FL    Altamon… Surprise     Fear          Happiness     male   black\n10 37243882 IA    Perry    Happiness    Happiness     Anger         male   white\n# ℹ 1,835 more rows\n# ℹ 4 more variables: target <chr>, emotion <chr>, image_race <chr>,\n#   image_gender <chr>\n\n\n\neas_data <- read_excel(\"data/MICROSOFT_DATASET.xlsx\")\n\neas_data <- eas_data |>\n  mutate(\n    race = case_when(\n      str_sub(Target, 1, 1) == \"A\" ~ \"Asian\",\n      str_sub(Target, 1, 1) == \"B\" ~ \"Black\",\n      str_sub(Target, 1, 1) == \"L\" ~ \"Latino\",\n      str_sub(Target, 1, 1) == \"W\" ~ \"White\"\n    ),\n    gender = case_when(\n      str_sub(Target, 2, 2) == \"M\" ~ \"Male\",\n      str_sub(Target, 2, 2) == \"F\" ~ \"Female\"\n    )\n  )\n\n\nwrite.csv(eas_data, \"data/eas_data.csv\")\n\neas_data\n\n# A tibble: 1,207 × 12\n   Target Emotion SADNESS NEUTRAL CONTEMPT DISGUST ANGER SURPRISE  FEAR\n   <chr>  <chr>     <dbl>   <dbl>    <dbl>   <dbl> <dbl>    <dbl> <dbl>\n 1 AF-205 N         0       1        0           0     0        0     0\n 2 AF-208 N         0       1        0           0     0        0     0\n 3 AF-249 N         0.011   0.983    0.005       0     0        0     0\n 4 AF-220 N         0.001   0.993    0.001       0     0        0     0\n 5 AF-235 N         0.002   0.994    0.003       0     0        0     0\n 6 AF-227 N         0       1        0           0     0        0     0\n 7 AF-242 N         0.035   0.964    0           0     0        0     0\n 8 AF-234 N         0.02    0.979    0.001       0     0        0     0\n 9 AF-247 N         0.002   0.954    0.002       0     0        0     0\n10 AF-256 N         0.188   0.811    0.001       0     0        0     0\n# ℹ 1,197 more rows\n# ℹ 3 more variables: HAPPINESS <dbl>, race <chr>, gender <chr>\n\n\n\neas_data_clean <- eas_data |>\n  mutate(Emotion = case_when(\n    Emotion == \"N\" ~ \"NEUTRAL\",\n    Emotion %in% c(\"HO\", \"HC\") ~ \"HAPPINESS\",\n    Emotion == \"A\" ~ \"ANGER\",\n    Emotion == \"F\" ~ \"FEAR\",\n    TRUE ~ Emotion\n  ))\n\nwrite.csv(eas_data_clean, \"data/eas_data_clean.csv\")\n\neas_data_clean\n\n# A tibble: 1,207 × 12\n   Target Emotion SADNESS NEUTRAL CONTEMPT DISGUST ANGER SURPRISE  FEAR\n   <chr>  <chr>     <dbl>   <dbl>    <dbl>   <dbl> <dbl>    <dbl> <dbl>\n 1 AF-205 NEUTRAL   0       1        0           0     0        0     0\n 2 AF-208 NEUTRAL   0       1        0           0     0        0     0\n 3 AF-249 NEUTRAL   0.011   0.983    0.005       0     0        0     0\n 4 AF-220 NEUTRAL   0.001   0.993    0.001       0     0        0     0\n 5 AF-235 NEUTRAL   0.002   0.994    0.003       0     0        0     0\n 6 AF-227 NEUTRAL   0       1        0           0     0        0     0\n 7 AF-242 NEUTRAL   0.035   0.964    0           0     0        0     0\n 8 AF-234 NEUTRAL   0.02    0.979    0.001       0     0        0     0\n 9 AF-247 NEUTRAL   0.002   0.954    0.002       0     0        0     0\n10 AF-256 NEUTRAL   0.188   0.811    0.001       0     0        0     0\n# ℹ 1,197 more rows\n# ℹ 3 more variables: HAPPINESS <dbl>, race <chr>, gender <chr>\n\n\n\nchoice_correct_eas <- eas_data_clean |>\n  rowwise() |>\n  mutate(\n    value = max(c_across(SADNESS:HAPPINESS)),\n     PredictedEmotion = names(eas_data_clean)\n      [which.max(c_across(SADNESS:HAPPINESS)) + 2]             \n    ) |>\n  select(-SADNESS:-value) |>\n  mutate(Correct = Emotion == PredictedEmotion)\n\n\nwrite.csv(choice_correct_eas, \"data/choice_correct_eas.csv\")\n\n\n\nOther appendicies (as necessary)\nThese code chunks show the results from analyzing gender and exploring if there is any gender bias. Interestingly, the pvalues showed that there is a significant difference in the guessing accuracy for the emotion “fear” in females compared to males(pvalue = 0.00085). Fear is often confused with surprise and happinessin females. Although these findings are interesting, they are not very relevant to our study.\n\nprobs <- eas_data |>\n  select(SADNESS:HAPPINESS)\n\npreds <- colnames(probs)[apply(probs, 1, which.max)]\n\ndf <- eas_data |>\n  mutate(actual = factor(Emotion, levels= c(\"N\" , \"HC\" ,\"A\",  \"HO\" ,\"F\" ), \n                          labels=c('NEUTRAL', 'HAPPINESS', 'ANGER', 'HAPPINESS', 'FEAR'))) |>\n  mutate(predicted = preds) |>\n  mutate(accuracy = predicted == actual ) \n\ndf |>\n  filter(race %in% c('Black', 'White')) |>\n  count(gender, predicted, actual) |>\n  complete(predicted, actual, gender, fill=list(n=0)) |>\n  ggplot(aes(x=actual, y=predicted, fill=n)) + geom_tile() + facet_wrap(~gender) +\n  geom_text(aes(label=n), color='white')\n\n\n\n\n\nres <- data.frame()\nfor (cat in unique(df$actual)){\n  cur <- df |>\n    filter(actual == cat, race %in% c('Black', 'White'))\n  comp <- chisq.test(table(cur$gender, cur$predicted))\n  res <- bind_rows(res, data.frame(category=cat, statisitcs=comp$statistic, pvalue=comp$p.value))\n}\n\nWarning in chisq.test(table(cur$gender, cur$predicted)): Chi-squared\napproximation may be incorrect\n\nWarning in chisq.test(table(cur$gender, cur$predicted)): Chi-squared\napproximation may be incorrect\n\nWarning in chisq.test(table(cur$gender, cur$predicted)): Chi-squared\napproximation may be incorrect\n\nWarning in chisq.test(table(cur$gender, cur$predicted)): Chi-squared\napproximation may be incorrect\n\nres <- res |>\n  tibble::rownames_to_column() |> \n  select(-rowname)\nres\n\n   category statisitcs       pvalue\n1   NEUTRAL   4.857732 0.0881367248\n2 HAPPINESS   1.044670 0.3067378368\n3     ANGER   7.539811 0.1834877455\n4      FEAR  22.834811 0.0008537349"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Racial Bias in EAS",
    "section": "",
    "text": "Research question(s)\nResearch question(s). State your research question (s) clearly.\nIs there an inherent racial bias in image tagging emotion analyses services in their reading of emotions as compared to a real person reading emotion?\nDoes this corroborate any previous research on the bias towards black individuals erroneously being perceived as angry or hostile through their expressions? What implications does this have on visual cognitive services if so?\n\n\nData collection and cleaning\nHave an initial draft of your data cleaning appendix. Document every step that takes your raw data file(s) and turns it into the analysis-ready data set that you would submit with your final project. Include text narrative describing your data collection (downloading, scraping, surveys, etc) and any additional data curation/cleaning (merging data frames, filtering, transformations of variables, etc). Include code for data curation/cleaning, but not collection.\nCleaning code done and shown in appenicies.qmd\n\nDownloaded the data and read it in from Excel sheet\nDropped unnecessary variables that do not aid us for the question\nRenamed the rest of the columns and added them to clean_person_data\nMutated data frame and cleaned race and gender’s values\nParsed through url’s and coded two new variables to match the EAS data\nDropped url variable\nWrote both data frames to csv for new clean data tables\n\nFor the computer data:\n\nChanged the code names to emotions to match\n\n\n\nData description\nIn the first data set we are using, person_data_clean, each row represents a different person that is tasked with describing the facial emotion of a picture of someone. The columns of this data set include information about the evaluator, including the city they are from, their race, and their gender. Other columns include their first and second choice of emotion that they would use to describe the person in the photo, as well as the emotion that they would not use. The second data set that we are using, eas_data, has a column has the value of the correct emotion of the picture, as well as the portion of evaluators who classified the emotion expressed in the picture for 7 different emotions.\nThese data sets were created in order to determine if popular vision-based cognitive software that infers emotion from a person’s face perpetuates racial and gender sterotypes concerning emotion. This is espically relevant in modern society where this type of imaging technology is used in a wide range of applications, from social networks and smartphone applications to real time security. The creation of this data set was funded by Harvard University.\n\n\nData limitations\nThere are a few potential problems with the data set. The first is that the we are looking at two different data sets and want to base our conclusion off of their comparison. However, the two datasets are very different so they are difficult to compare. Second, there are a lot of columns in the dataset making it difficult to decide which ones are important when creating visualizations and analysing the data. The data also includes a link to the image shown in the study rather than a description in the dataset; in fact, the person_data dataset only includes the race of the participant and not of the person in the image which may make it difficult to make conclusions on racial bias.\n\n\nExploratory data analysis\nPerform an (initial) exploratory data analysis.\n\nlibrary(readxl)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(skimr)\n\nperson_data_clean <- read.csv(\"data/person_data_clean.csv\")\neas_data <- read.csv(\"data/eas_data.csv\")\neas_data_clean <- read.csv(\"data/eas_data_clean.csv\")\nchoice_correct_eas <- read.csv(\"data/choice_correct_eas.csv\")\n\n\neas_data <- read_excel(\"data/MICROSOFT_DATASET.xlsx\")\n\ncorrect_count <- choice_correct_eas |>\n  filter(Correct) |>\n  count(Emotion)\n\nggplot(correct_count, aes(x = Emotion, y = n, fill = Emotion)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Number of Correct Guesses per Emotion\",\n       x = \"Emotion\",\n       y = \"Correct Guesses\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nQuestions for reviewers\nList specific questions for your peer reviewers and project mentor to answer in giving you feedback on this phase.\n\nHow do we go about comparing the two data sets and making conclusions about bias?\nShould we try to combine the two data sets or leave them separate?\nDo you think that answering our research questions is feasible based on this data?"
  },
  {
    "objectID": "pre-registration.html",
    "href": "pre-registration.html",
    "title": "Project title",
    "section": "",
    "text": "Analysis #1\nNull Hypothesis: There is not a significantly higher proportion of “false positives” for incorrect angry decisions made by the EAS for black men as compared to real people making the decision\nAlternative Hypothesis: There is a significantly higher proportion of “false positives” for incorrect angry decisions made by the EAS for black men as compared to real people making the decision.\nAnalysis: We can first perform calculations to filter and find the proportion of incorrect false positive angry choices in both the EAS data set and the real person responses. Then we can take these proportions and perform a two sample proportion test or bootstrapping in order to see if the p value is less than 0.05 to show significance.\nVisualization/Presentation: We can then take the two proportions and show them side by side on bar charts, and if there is a p value we can display the value to indicate a significant difference while showing if there is a true statistical difference in proportions between the EAS and real deciders.\n\n\nAnalysis #2\nNull Hypothesis: There is no significant difference in the emotion classification accuracy between image tagging emotion analysis services and human evaluators for different emotions.\nAlternative Hypothesis: There is a significant difference in the emotion classification accuracy between image tagging emotion analysis services and human evaluators for different emotions.\nAnalysis: First, we will calculate the emotion classification accuracy for both image tagging emotion analysis services and human evaluators for each emotion (e.g., happiness, anger, fear, etc.). We will then calculate the 95% confidence interval using bootstrapping or other suitable methods. If the confidence interval for the difference in accuracy for any emotion does not contain zero, we can reject the null hypothesis and conclude that there is a significant difference in emotion classification accuracy between image tagging emotion analysis services and human evaluators for that particular emotion.\nVisualization/Presentation: To visualize the results, we can create a graph comparing the emotion classification accuracy for image tagging emotion analysis services for each emotion. The chart will display the accuracy of each group for every emotion, allowing for an easy comparison of their performance.\nNote: The preregistered analysis 1 and 2 were combined to both go into analysis 1, where we conducted a 95% confidence interval and performed a two sample prop test. This was decided before we actually conducted the tests, we just decided that the pre-registered analysis two just didn’t end up making sense for what we were trying to conclude and it made more sense to combine both and create another analysis.\n\n\nAnalysis #3\n(pre-registration for analysis 2)\nNull Hypothesis: There is no statistically significant difference between actual and predicted categories for race.\nAlternative Hypothesis: There is a statistically significant difference between actual and predicted categories for race.\nAnalysis: Conduct a chi-squared test to determine a difference in predicted vs actual for each emotion in the EAS data.\nVisualization: Make a heat map to show the amount of times that an emotion was guessed correctly or what emotion it was mostly confused with for both black and white."
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "Image Tagging Bias",
    "section": "",
    "text": "Today we will be investigating if there is an inherent bias in image tagging emotion analysis services (EAS) perpetuating racial stereotypes concerning emotion.\n\nIdentifying and addressing racial bias in emotion analysis services is crucial for ensuring fairness, transparency, and accuracy in AI applications.\nFindings from this study can inform policy and industry guidelines, leading to more ethical and inclusive AI systems."
  },
  {
    "objectID": "presentation.html#introduce-the-data",
    "href": "presentation.html#introduce-the-data",
    "title": "Image Tagging Bias",
    "section": "Introduce the data",
    "text": "Introduce the data\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "presentation.html#highlights-from-eda",
    "href": "presentation.html#highlights-from-eda",
    "title": "Image Tagging Bias",
    "section": "Highlights from EDA",
    "text": "Highlights from EDA\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2"
  },
  {
    "objectID": "presentation.html#inferencemodelingother-analysis",
    "href": "presentation.html#inferencemodelingother-analysis",
    "title": "Image Tagging Bias",
    "section": "Inference/modeling/other analysis",
    "text": "Inference/modeling/other analysis\nFor our first hypothesis, we wanted to determine if there was a significantly higher proportion of “false positive” angry decisions made by the Emotion Analysis Service as compared to real people making the same decisions.\n\n\nNull Hypothesis: There is not a significantly higher proportion of “false positives” for incorrect angry decisions made by the EAS for black men as compared to real people making the decision\n\\[\nH_0: p_1 - p_2 = 0\n\\]\n\nAlternative Hypothesis: There is a significantly higher proportion of “false positives” for incorrect angry decisions made by the EAS for black men as compared to real people making the decision.\n\\[\nH_A: p_1 - p_2 \\neq 0\n\\]\n\n\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(length(type1_eas), length(type1_person)) out of c(length(eas_data_clean), length(person_data_clean))\nX-squared = 0.15476, df = 1, p-value = 0.694\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.6094210  0.3017287\nsample estimates:\n   prop 1    prop 2 \n0.3846154 0.5384615"
  },
  {
    "objectID": "presentation.html#inferencemodelingother-analysis-1",
    "href": "presentation.html#inferencemodelingother-analysis-1",
    "title": "Image Tagging Bias",
    "section": "Inference/modeling/other analysis",
    "text": "Inference/modeling/other analysis\nHypothesis: There will be a difference in the emotion guessing accuracy when comparing black vs. white."
  },
  {
    "objectID": "presentation.html#conclusions-future-work",
    "href": "presentation.html#conclusions-future-work",
    "title": "Image Tagging Bias",
    "section": "Conclusions + future work",
    "text": "Conclusions + future work\n\nTo conclude, we failed to reject the null hypothesis for both analysis. We found that there was not enough evidence to conclude a significant difference in the proportion of false positive angry predictions between the two populations or between the actual and predicted categories for race.\nDespite these results, it is important to consider the flaws in the data such as the lack of data for the Asian and Latino categories.\nIn the future, it would be interesting to look further into gender and explore any potential bias. For example, would there have been a more significant difference if we compared the results for black women compared to white women?"
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Project title",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\nlibrary(rvest)\nlibrary(lubridate)\nlibrary(robotstxt)\nlibrary(readxl)"
  },
  {
    "objectID": "proposal.html#introduction-and-data",
    "href": "proposal.html#introduction-and-data",
    "title": "Project title",
    "section": "Introduction and data",
    "text": "Introduction and data\n\nIdentify the source of the data.\nThe source of this data is the College Scorecard of the U.S. Department of Education.\nState when and how it was originally collected (by the original data curator, not necessarily how you found the data).\nEvery year the U.S. Department of Education collects the aggregate data, which includes various qualitative and quanitiative variables, on every institute of higher education. This data was obtained by the U.S. Department of Education through federal reporting from the institutions, data on federal financial aid, and tax information. Much of the data was also obtained from data reported to the IPEDS, the Integrated Post-secondary Education Data System. The data set college_information contains the values of these variables for each institute of higher education in the U.S. for the 2020-21 school year, the most recent data collected.\nWrite a brief description of the observations.\nEach row in the data set represents each institute of higher education in California. The data set also includes the city that the institutions are in, their zip code, the larger collection of schools that the university is apart of, the website of the university, and various identification codes for the university. The variables within the data set include those that store the values of the SAT and ACT test scores of students admitted into the university, the admission rate of the university, average cost of attendance of the university, and more."
  },
  {
    "objectID": "proposal.html#research-question",
    "href": "proposal.html#research-question",
    "title": "Project title",
    "section": "Research question",
    "text": "Research question\n\nA well formulated research question. (You may include more than one research question if you want to receive feedback on different ideas for your project. However, one per data set is required.)\nWhat is the relationship between the average SAT score of admitted Californian students and the average amount of student debt students have upon graduation for both private and public institutions of higher learning for the 2020-21 academic year?\nA description of the research topic along with a concise statement of your hypotheses on this topic.\nThe research topic explores if there is a relationship between the average SAT score for California students admitted into the university and the cumulative student debt that these students have when they leave the university, either by graduating or withdrawing from it. It also compares this relationship to the structure of the governance of the school (ie. whether it is public, private nonprofit and private for-profit.) We hypothesize that intuitions that admit students with higher SAT scores will have a lower student.\nIdentify the types of variables in your research question. Categorical? Quantitative?\nThe variables in our research question are both quantitative and categorical. The qualitative variable in our research question is the type of governance structure of the institution- public, private nonprofit and private for-profit. The quantitative variables in our research question are the average SAT score of the students admitted into the university, as well as cumulative median student debt, meaning the median loan debt accumulated at the institution by all student borrowers of federal loans who separate (either graduate or withdraw from the university) in a given fiscal year."
  },
  {
    "objectID": "proposal.html#glimpse-of-data",
    "href": "proposal.html#glimpse-of-data",
    "title": "Project title",
    "section": "Glimpse of data",
    "text": "Glimpse of data\n\n# add code here\nsat_scorecard <- read_csv(\"data/school_scores.xlsx\")\n\nMultiple files in zip: reading '[Content_Types].xml'\nRows: 1 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nskimr::skim(sat_scorecard)\n\n\nData summary\n\n\nName\nsat_scorecard\n\n\nNumber of rows\n1\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\n\n0\n1\n1111\n1111\n0\n1\n0"
  },
  {
    "objectID": "proposal.html#introduction-and-data-1",
    "href": "proposal.html#introduction-and-data-1",
    "title": "Project title",
    "section": "Introduction and data",
    "text": "Introduction and data\n\nIdentify the source of the data.\nThe source of the data is the Harvard Dataverse.\nState when and how it was originally collected (by the original data curator, not necessarily how you found the data).\nThese data sets were found in two ones, one being crowd-sourcing human participants to respond on what emotions they believed a displayed face to have. The second was through showing faces to different Emotion Analyses Services to see what their predictions on the emotion of the given face was.\nWrite a brief description of the observations.\nEach observation represents one image being shown, and the corresponding response given by either a real person or an EAS. The variables include different emotions that are response options, and a demographic variable that indicates the race and gender of the displayed image. The variables in the EAS dataset represent the demographic of the picture with 7 different emotion options that the service will assign values of probability to. The variables in the real person data set again represent the demographic data of the displayed person, alongside the 1st and 2nd choice emotion values that the person will choose, and the opposite emotion they believe the face to hold."
  },
  {
    "objectID": "proposal.html#research-question-1",
    "href": "proposal.html#research-question-1",
    "title": "Project title",
    "section": "Research question",
    "text": "Research question\n\nA well formulated research question. (You may include more than one research question if you want to receive feedback on different ideas for your project. However, one per data set is required.)\nA description of the research topic along with a concise statement of your hypotheses on this topic.\nIdentify the types of variables in your research question. Categorical? Quantitative?\n\nIs there an inherent racial bias in image tagging emotion analyses services in their reading of emotions as compared to a real person reading emotion? Does this corroborate any previous research on the bias towards black individuals erroneously being perceived as angry or hostile through their expressions? What implications does this have on visual cognitive services if so?\nThe research topic is on algorithmic discrimination in image tagging emotion analyses services, and we hypothesize that there will be a significant difference in the computer tagging black individuals as more angry as opposed to real subjects. The variables included in the research are categorical, with some of the categorical variables having quantitative values."
  },
  {
    "objectID": "proposal.html#glimpse-of-data-1",
    "href": "proposal.html#glimpse-of-data-1",
    "title": "Project title",
    "section": "Glimpse of data",
    "text": "Glimpse of data\n\n# add code here\neas_data <- read_excel(\"data/MICROSOFT_DATASET.xlsx\")\nskimr::skim(eas_data)\n\n\nData summary\n\n\nName\neas_data\n\n\nNumber of rows\n1207\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nTarget\n0\n1\n6\n6\n0\n597\n0\n\n\nEmotion\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSADNESS\n0\n1\n0.02\n0.08\n0\n0\n0.00\n0.00\n0.96\n▇▁▁▁▁\n\n\nNEUTRAL\n0\n1\n0.56\n0.46\n0\n0\n0.84\n0.99\n1.00\n▆▁▁▁▇\n\n\nCONTEMPT\n0\n1\n0.02\n0.06\n0\n0\n0.00\n0.00\n0.68\n▇▁▁▁▁\n\n\nDISGUST\n0\n1\n0.01\n0.04\n0\n0\n0.00\n0.00\n0.56\n▇▁▁▁▁\n\n\nANGER\n0\n1\n0.04\n0.15\n0\n0\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nSURPRISE\n0\n1\n0.07\n0.22\n0\n0\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nFEAR\n0\n1\n0.02\n0.11\n0\n0\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nHAPPINESS\n0\n1\n0.27\n0.43\n0\n0\n0.00\n0.78\n1.00\n▇▁▁▁▃\n\n\n\n\nperson_data <- read_excel(\"data/PERSON_DATASET.xlsx\")\nskimr::skim(person_data)\n\n\nData summary\n\n\nName\nperson_data\n\n\nNumber of rows\n1845\n\n\nNumber of columns\n23\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n11\n\n\nlogical\n6\n\n\nnumeric\n4\n\n\nPOSIXct\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\n_channel\n0\n1.00\n4\n14\n0\n16\n0\n\n\n_country\n0\n1.00\n3\n3\n0\n1\n0\n\n\n_region\n106\n0.94\n2\n2\n0\n44\n0\n\n\n_city\n111\n0.94\n3\n17\n0\n230\n0\n\n\n_ip\n0\n1.00\n10\n15\n0\n310\n0\n\n\n1st_choice\n0\n1.00\n4\n12\n0\n8\n0\n\n\n2nd_choice\n0\n1.00\n4\n12\n0\n8\n0\n\n\ni_would_not_use\n0\n1.00\n4\n12\n0\n8\n0\n\n\nselect_your_gender\n0\n1.00\n4\n9\n0\n5\n0\n\n\nselect_your_race\n0\n1.00\n4\n6\n0\n8\n0\n\n\nimage_url\n0\n1.00\n38\n39\n0\n610\n0\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\n_tainted\n0\n1\n0\nFAL: 1845\n\n\n1st_choice_gold\n1845\n0\nNaN\n:\n\n\n2nd_choice_gold\n1845\n0\nNaN\n:\n\n\ni_would_not_use_gold\n1845\n0\nNaN\n:\n\n\nselect_your_gender_gold\n1845\n0\nNaN\n:\n\n\nselect_your_race_gold\n1845\n0\nNaN\n:\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\n_unit_id\n0\n1\n2.276232e+09\n177.02\n2276231480.0\n2.276232e+09\n2.276232e+09\n2276231940.0\n2276232092\n▇▇▇▇▇\n\n\n_id\n0\n1\n4.896590e+09\n36899584.69\n4780811066.0\n4.910562e+09\n4.910811e+09\n4911186464.0\n4912209025\n▁▁▁▁▇\n\n\n_trust\n0\n1\n7.500000e-01\n0.12\n0.4\n6.700000e-01\n7.300000e-01\n0.8\n1\n▁▃▇▅▃\n\n\n_worker_id\n0\n1\n3.474014e+07\n13541771.24\n1853182.0\n2.852137e+07\n4.154501e+07\n45205782.0\n45331060\n▁▁▁▂▇\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\n_created_at\n0\n1\n2019-05-07 12:44:00\n2019-06-22 12:08:14\n2019-06-21 14:37:59\n1827\n\n\n_started_at\n0\n1\n2019-05-07 12:42:00\n2019-06-22 12:04:48\n2019-06-21 14:37:42\n1825"
  },
  {
    "objectID": "proposal.html#introduction-and-data-2",
    "href": "proposal.html#introduction-and-data-2",
    "title": "Project title",
    "section": "Introduction and data",
    "text": "Introduction and data\n\nIdentify the source of the data.\nThe function reads HTML from Zillow URLs and extracts information on homes that have recently been sold in Ithaca, NY. The data was on March 15, 2023.\nState when and how it was originally collected (by the original data curator, not necessarily how you found the data).\nThe data was scraped from the Zillow website on March 15, 2023.\nWrite a brief description of the observations.\nThe observations in the data include the date of sale, sale price, number of bedrooms, number of bathrooms, square footage, and realtor for each home sold. The data covers 10 pages of results from Zillow, with each page containing information on up to 40 homes."
  },
  {
    "objectID": "proposal.html#research-question-2",
    "href": "proposal.html#research-question-2",
    "title": "Project title",
    "section": "Research question",
    "text": "Research question\n\nA well formulated research question. (You may include more than one research question if you want to receive feedback on different ideas for your project. However, one per data set is required.)\nDoes the number of bedrooms or bathrooms have a stronger effect on the sale price of homes in Ithaca, NY?\nA description of the research topic along with a concise statement of your hypotheses on this topic.\nThe research topic is the relationship between the number of bedrooms and bathrooms and the sale price of homes in Ithaca, NY. This topic falls within the broader field of real estate research, and the aim of this analysis is to identify which factor - the number of bedrooms or bathrooms - has a stronger effect on home sale prices.\n\nHypothesis: The number of bedrooms has a stronger effect on the sale price of homes in Ithaca, NY than the number of bathrooms.\n\nIdentify the types of variables in your research question. Categorical? Quantitative?\nThe independent variables in this research question are the number of bedrooms and the number of bathrooms, both of which are quantitative variables. The dependent variable is the sale price of the homes, which is also a quantitative variable."
  },
  {
    "objectID": "proposal.html#glimpse-of-data-2",
    "href": "proposal.html#glimpse-of-data-2",
    "title": "Project title",
    "section": "Glimpse of data",
    "text": "Glimpse of data\n\n# add code here\nreal_estate <- read_csv(\"data/real_estate.csv\")\n\nRows: 9129 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): data.date, data.owned or leased, data.status, data.type, location....\ndbl  (5): data.parking spaces, location.congressional district, location.reg...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nskimr::skim(real_estate)\n\n\nData summary\n\n\nName\nreal_estate\n\n\nNumber of rows\n9129\n\n\nNumber of columns\n15\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ndata.date\n0\n1\n1\n10\n0\n1380\n0\n\n\ndata.owned or leased\n0\n1\n5\n6\n0\n2\n0\n\n\ndata.status\n0\n1\n6\n14\n0\n3\n0\n\n\ndata.type\n0\n1\n4\n9\n0\n3\n0\n\n\nlocation.id\n0\n1\n6\n6\n0\n9129\n0\n\n\ndata.disabilities.ADA Accessible\n0\n1\n2\n12\n0\n3\n0\n\n\nlocation.address.city\n0\n1\n3\n25\n0\n1964\n0\n\n\nlocation.address.county\n0\n1\n4\n30\n0\n947\n0\n\n\nlocation.address.line 1\n0\n1\n3\n57\n0\n8076\n0\n\n\nlocation.address.state\n0\n1\n2\n2\n0\n56\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata.parking spaces\n0\n1\n50.95\n189.31\n0\n0\n8\n39\n6600\n▇▁▁▁▁\n\n\nlocation.congressional district\n22\n1\n14.83\n23.47\n0\n2\n6\n16\n98\n▇▁▁▁▁\n\n\nlocation.region id\n0\n1\n6.22\n2.90\n1\n4\n7\n9\n11\n▆▇▆▆▅\n\n\ndata.disabilities.ansi usable\n0\n1\n33351.82\n94770.75\n0\n2535\n7629\n21353\n1989117\n▇▁▁▁▁\n\n\nlocation.address.zip\n0\n1\n466892590.74\n324349643.23\n605\n200242104\n432151052\n785216762\n999019998\n▇▇▃▆▆"
  },
  {
    "objectID": "presentation.html#introduce-the-topic-and-motivation",
    "href": "presentation.html#introduce-the-topic-and-motivation",
    "title": "Image Tagging Bias",
    "section": "Introduce the topic and motivation",
    "text": "Introduce the topic and motivation\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "presentation.html#topic-and-motivation",
    "href": "presentation.html#topic-and-motivation",
    "title": "Image Tagging Bias",
    "section": "Topic and Motivation",
    "text": "Topic and Motivation\nToday we will be investigating if there is an inherent bias in image tagging emotion analysis services (EAS) perpetuating racial stereotypes concerning emotion.\n\nIdentifying and addressing racial bias in emotion analysis services is crucial for ensuring fairness, transparency, and accuracy in AI applications.\nFindings from this study can inform policy and industry guidelines, leading to more ethical and inclusive AI systems."
  },
  {
    "objectID": "presentation.html#the-data",
    "href": "presentation.html#the-data",
    "title": "Image Tagging Bias",
    "section": "The Data",
    "text": "The Data\n\nThe dataset consists of raw data collected from Emotion Analysis Services (EAS) and Crowdsourcing (Crowdworkers from the Appen Platform targeting US participants).\nThe Chicago Face Database (CFD) is used as the primary dataset for testing the behavior of the target EAS.\nBoth humans and EAS were shown images of people showing an emotion and were asked to guess what emotion that was.\n\n\n\n# A tibble: 4 × 2\n  race   `mean(accuracy)`\n  <chr>             <dbl>\n1 Asian             1    \n2 Black             0.749\n3 Latino            0.991\n4 White             0.759\n\n\n     race    actual   n\n1   Asian   NEUTRAL 109\n2   Black   NEUTRAL 197\n3   Black HAPPINESS 164\n4   Black     ANGER  82\n5   Black      FEAR  83\n6  Latino   NEUTRAL 108\n7   White   NEUTRAL 183\n8   White HAPPINESS 143\n9   White     ANGER  72\n10  White      FEAR  66"
  },
  {
    "objectID": "presentation.html#highlights-from-data-analysis",
    "href": "presentation.html#highlights-from-data-analysis",
    "title": "Image Tagging Bias",
    "section": "Highlights from Data Analysis",
    "text": "Highlights from Data Analysis\nFirst, we wanted to explore some more general patterns in the data through linear regression and bar plots. Below, we plotted the relationship between emotion and race(none, as expected), as well and the distribution of emotions for both the human and EAS datasets to observe the similarities."
  },
  {
    "objectID": "presentation.html#analysis-2",
    "href": "presentation.html#analysis-2",
    "title": "Image Tagging Bias",
    "section": "Analysis 2",
    "text": "Analysis 2\nHere, we are visualizing the relationship between actual and predicted categories for different races using heatmaps. We also performed a chi-squared test to investigate if there is a statistically significant difference between actual and predicted categories for race. We hypothesized that the EAS would be bias and there would be differences in the results when comparing white vs. black.\n\n\nNull Hypothesis: There is no statistically significant difference between actual and predicted categories for race.\n\\[\nH_0: p_1 - p_2 = 0\n\\]\n\nAlternative Hypothesis: There is a statistically significant difference between actual and predicted categories for race.\n\\[\nH_A: p_1 - p_2 \\neq 0\n\\]\n\n\n\n\n\n\n\n\n   category statisitcs     pvalue\n1   NEUTRAL  4.7065990 0.09505501\n2 HAPPINESS  0.9025027 0.34211153\n3     ANGER  4.0391307 0.54379596\n4      FEAR  7.9138975 0.24447953"
  },
  {
    "objectID": "presentation.html#analysis-1",
    "href": "presentation.html#analysis-1",
    "title": "Image Tagging Bias",
    "section": "Analysis 1",
    "text": "Analysis 1\nFor our first hypothesis, we wanted to determine if there was a significantly higher proportion of “false positive” angry decisions made by the Emotion Analysis Service as compared to real people making the same decisions.\n\n\nNull Hypothesis: There is not a significantly higher proportion of “false positives” for incorrect angry decisions made by the EAS for black men as compared to real people making the decision\n\\[\nH_0: p_1 - p_2 = 0\n\\]\n\nAlternative Hypothesis: There is a significantly higher proportion of “false positives” for incorrect angry decisions made by the EAS for black men as compared to real people making the decision.\n\\[\nH_A: p_1 - p_2 \\neq 0\n\\]\n\n\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(length(type1_eas), length(type1_person)) out of c(length(eas_data_clean), length(person_data_clean))\nX-squared = 0.15476, df = 1, p-value = 0.694\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.6094210  0.3017287\nsample estimates:\n   prop 1    prop 2 \n0.3846154 0.5384615"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TEAM NAME",
    "section": "",
    "text": "Add project abstract here."
  }
]